{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import spatial\n",
    "from random import sample\n",
    "import copy\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 50000 words from glove.42B.300d.zip were written to top_50000.txt.\n",
    "\n",
    "# Load all data from massive embeddings file. You only need to do this once.\n",
    "# Only adds nouns and proper nouns to \n",
    "embeddings_all = {}\n",
    "with open(\"./top_50000.txt\", 'r', encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        tag = nltk.pos_tag([word])[0][1]\n",
    "        if tag in ['NN', 'NNP'] and len(word) != 1:\n",
    "            vector = np.asarray(values[1:], \"float32\")\n",
    "            embeddings_all[word] = vector\n",
    "\n",
    "# Load list of codenames words.\n",
    "codenames_words = []\n",
    "with open(\"./codenames_words.txt\", 'r') as f:\n",
    "    for line in f:\n",
    "        codenames_words.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for choosing smaller sets of embeddings. \n",
    "def embeddings_size(embeddings_size, embeddings_all):\n",
    "    #embeddings_size is the length of the subset of embeddings\n",
    "    #embeddings_all is the embeddings from 50k top words\n",
    "    embeddings = {}\n",
    "    for x in list(embeddings_all)[0:embeddings_size]:\n",
    "        embeddings[x] = embeddings_all[x]\n",
    "    \n",
    "    #adds words from codenames that don't make it in the top XX words\n",
    "    for item in codenames_words:\n",
    "        if item not in embeddings:\n",
    "            try: \n",
    "                embeddings[item] = embeddings_all[item]\n",
    "            except KeyError: \n",
    "                codenames_words.remove(item)\n",
    "            \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings_1000 = embeddings_size(1000, embeddings_all)\n",
    "#embeddings_5000 = embeddings_size(5000, embeddings_all)\n",
    "#embeddings_10000 = embeddings_size(10000, embeddings_all)\n",
    "embeddings_25000 = embeddings_size(25000, embeddings_all)\n",
    "embeddings_50000 = embeddings_size(50000, embeddings_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "def distance(embeddings, word, reference):\n",
    "    return spatial.distance.cosine(embeddings[word], embeddings[reference])\n",
    "\n",
    "def closest_words(embeddings, reference):\n",
    "    return sorted(embeddings.keys(), key=lambda w: distance(embeddings, w, reference))\n",
    "\n",
    "# Scoring function for generating clues based on sum of cosine distance\n",
    "def goodness(embeddings, word, answers):\n",
    "    if word in answers: return -999\n",
    "    return sum([distance(embeddings, word, a) for a in answers])\n",
    "\n",
    "# Chooses top candidates\n",
    "def candidates(embeddings, answers, size=40):\n",
    "    best = sorted(embeddings.keys(), key=lambda w: goodness(embeddings, w, answers))\n",
    "    #res = [(str(i + 1), \"{0:.2f}\".format(minimax(embeddings, w, answers)), w) \n",
    "           #for i, w in enumerate(sorted(best[:250], key=lambda w: -1 * minimax(embeddings, w, answers))[:size])]\n",
    "    \n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to help with word + scores (not necessary for data collection)...\n",
    "from itertools import zip_longest\n",
    "\n",
    "def grouper(n, iterable, fillvalue=None):\n",
    "    args = [iter(iterable)] * n\n",
    "    return zip_longest(fillvalue=fillvalue, *args)\n",
    "\n",
    "def tabulate(data):\n",
    "    data = list(grouper(10, data))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations \n",
    "    \n",
    "# Generates a single example (good words, top ten clues)\n",
    "def generate_single_example(codenames_words, embeddings):\n",
    "    # Enforce threshold for 'good' words\n",
    "#     while True: \n",
    "#         good = sample(codenames_words, 3)\n",
    "#         combo_list = list(combinations(good, 2)) \n",
    "#         threshold_list = [distance(embeddings_all, item[0], item[1]) > 0.5 for item in combo_list]\n",
    "#         if all(good): \n",
    "#             break\n",
    "    good = sample(codenames_words, 3)\n",
    "        \n",
    "    clues = candidates(embeddings, good)\n",
    "    \n",
    "    # remove good words from clues\n",
    "    for item in good: \n",
    "        if item in clues:\n",
    "            clues.remove(item)\n",
    "                \n",
    "    return good, clues[:10]\n",
    "\n",
    "# Generates n examples and writes them to filename\n",
    "def write_n_examples(codenames_words, embeddings, n, filename): \n",
    "    output_file = open(filename, 'w')\n",
    "    \n",
    "    for i in range(n): \n",
    "        good, clues = generate_single_example(codenames_words, embeddings)\n",
    "        word_string = ', '.join(good + clues) + '\\n'\n",
    "        output_file.write(str(i) + '.' + word_string)\n",
    "    \n",
    "    output_file.close()\n",
    "    \n",
    "# write_n_examples(codenames_words, embeddings_1000, 10, 'test_examples.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Not necessary yet...\n",
    "def read_n_examples(filename):\n",
    "    '''Reads filename of good, bad, clues examples into a three-dimensional array. \n",
    "    To access the 0th example: data[0]\n",
    "    To access the good words (in list form) from the 1st example: data[1][0]\n",
    "        good words list - 0\n",
    "        bad words list - 1\n",
    "        clue words list - 2\n",
    "    To access the first clue word from the 1st example: data[1][2][0]'''\n",
    "    file = open(filename, 'r')\n",
    "    \n",
    "    data = []\n",
    "\n",
    "    for line in file: \n",
    "        full_list = line.strip('\\n').split(',')\n",
    "        good = full_list[:3]\n",
    "        clues = full_list[9:]\n",
    "        data.append([good, bad, clues])\n",
    "        \n",
    "    file.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_n_examples(codenames_words, embeddings_50000, 10, 'examples_50000_vocab_nobadwords.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write_n_examples(codenames_words, embeddings_5000, 100, 'examples_5000_vocab_nobadwords.txt')\n",
    "write_n_examples(codenames_words, embeddings_10000, 10, 'examples_10000_vocab_nobadwords.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
