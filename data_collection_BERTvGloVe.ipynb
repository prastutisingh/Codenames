{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import spatial\n",
    "from random import sample\n",
    "import copy\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import encoding dictionaries. i.e. bert_dict['cat'] = [0.5, -.4, 0.001,...]\n",
    "bert_dict = np.load('bert.npy', allow_pickle = True); bert_dict = bert_dict[()]\n",
    "glove_dict = np.load('glove.npy', allow_pickle = True); glove_dict = glove_dict[()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import list of codenames words\n",
    "codenames_words = []\n",
    "with open(\"./codenames_words.txt\", 'r') as f:\n",
    "    for line in f:\n",
    "        codenames_words.append(line.strip())\n",
    "        \n",
    "# Remove codenames words not in embeddings\n",
    "def removeInvalidCodenames(embeddings, codenames_words):\n",
    "    removed_words = []\n",
    "    for item in codenames_words:\n",
    "        if item not in embeddings:\n",
    "            removed_words.append(item)\n",
    "            codenames_words.remove(item)\n",
    "    #print('Words removed from codenames are', removed_words)\n",
    "    return codenames_words\n",
    "\n",
    "codenames_words_glove = removeInvalidCodenames(glove_dict, codenames_words)\n",
    "codenames_words = removeInvalidCodenames(bert_dict, codenames_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn BERT into indexable embeddings\n",
    "all_bert_words = list(bert_dict.keys())\n",
    "word_to_idx = dict(zip(all_bert_words, range(len(all_bert_words))))\n",
    "idx_to_word = dict(zip(range(len(all_bert_words)), all_bert_words))\n",
    "\n",
    "# Get indexes for codenames words\n",
    "codenames_words_idxs = np.array([word_to_idx[word] for word in codenames_words])\n",
    "\n",
    "# Get BERT embeddings as giant matrix\n",
    "bert_embedding = [bert_dict[word] for word in all_bert_words]\n",
    "bert_embedding = np.vstack(bert_embedding) # shape = (42000, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "# Returns list of random indexes from indexes of codenames words\n",
    "def generate_single_board(codenames_words_idxs): \n",
    "    return np.random.choice(codenames_words_idxs, 3)\n",
    "\n",
    "# Calculates prob. distribution (using softmax) over entire vocabulary for a single board\n",
    "def clue_dist_for_one_board(embedding, board, scoring = 'cosine', temperature = 1):\n",
    "    '''\n",
    "    embedding: V x [embedding_size = 768]\n",
    "    board: (3,) [indices of good words in all_bert_words]\n",
    "    scoring: either cosine or euclidean\n",
    "    temperature: sampling smoothing over prob. distribution\n",
    "    '''\n",
    "    \n",
    "    V, K = embedding.shape\n",
    "    board_embedding = embedding[board] # shape = (3 x 768)\n",
    "    \n",
    "    # Calculate score for each word in vocabulary using the specified scoring function\n",
    "    if scoring == 'euclidean': \n",
    "        score = -((board_embedding.reshape(1, 3, K) - embedding.reshape(V, 1, K)) ** 2).sum(2).sum(1) # shape V\n",
    "    else: # cosine\n",
    "        score = (np.matmul(board_embedding, embedding.T) / \n",
    "                 (np.matmul(np.linalg.norm(board_embedding, axis=1, keepdims=True), \n",
    "                            np.linalg.norm(embedding, axis=1, keepdims=True).T))).sum(0)\n",
    "    \n",
    "    # Penalize scores for good words\n",
    "    score[board] -= 1e6\n",
    "    \n",
    "    return softmax(score / temperature)\n",
    "\n",
    "# Returns the top clues for a certain board based on the probability distribution over the vocabulary\n",
    "def return_top_clues(embedding_matrix, board, scoring, p_value): \n",
    "    # probability distribution for one board\n",
    "    p = clue_dist_for_one_board(embedding_matrix, board, scoring, 15)\n",
    "    return [idx_to_word[word] for word in np.arange(p.shape[0])[p > p_value]]\n",
    "\n",
    "# Write n examples to txt file\n",
    "def generate_n_examples(n, embedding_matrix, scoring, p_value): \n",
    "    with open('examples.txt', 'w') as f: \n",
    "        for i in range(n): \n",
    "            board = generate_single_board(codenames_words_idxs)\n",
    "            clues = return_top_clues(bert_embedding, board, scoring, p_value)\n",
    "            if len(clues) > 10:\n",
    "                word_string = ', '.join([idx_to_word[idx] for idx in board] + clues[:20]) + '\\n'\n",
    "                f.write(str(i) + '.' + word_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_n_examples(100, bert_embedding, 'cosine', 0.000024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clue_dist_for_one_board_pt(embedding, board, scoring='cosine', temperature=1):\n",
    "    \"\"\"\n",
    "    Assume for now that the inputs are numpy matrices:\n",
    "    embedding: V x K matrix of floats\n",
    "    board: (3,) matrix of ints\n",
    "    \"\"\"\n",
    "    embedding_pt = torch.tensor(embedding, requires_grad=False)\n",
    "    board_pt = torch.tensor(board, requires_grad=False)\n",
    "    V, K = embedding.shape\n",
    "    \n",
    "    # let's try it \n",
    "    board_embedding_pt = embedding_pt[board_pt].view(1, 3 * K) # 3 x 768\n",
    "    board_embedding_big = board_embedding_pt.expand(V, 3 * K)\n",
    "    all_input = torch.concat([embedding_pt, board_embedding_big], dim=1) # (V x 3072)\n",
    "    \n",
    "    # deeplearning ML whoooo\n",
    "    neural_network_1 = nn.Linear(3072, 100)\n",
    "    neural_network_2 = nn.Linear(100, 1)\n",
    "    \n",
    "    output = neural_network_2(neural_network_1(all_input)) # V x 1\n",
    "    return torch.softmax(output / temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Linear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "board = generate_single_board(codenames_words_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNeuralNetwork(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(MyNeuralNetwork, self).__init__()\n",
    "        self.neural_network_1 = nn.Linear(3072, hidden_size)\n",
    "        self.neural_network_2 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return nn.Tanh(self.neural_network_2(nn.Tanh(self.neural_network_1(x))))5678"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(5000, 3072)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(mnn(x) == mnn.forward(x)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnn = MyNeuralNetwork(50)\n",
    "mnn(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnn = MyNeuralNetwork(50)\n",
    "\n",
    "def clue_dist_...\n",
    "    \n",
    "    output = mnn(all_input)\n",
    "    \n",
    "    return torch.softmax()\n",
    "\n",
    "def loss_function(p, gold_word):\n",
    "    return torch.nn.functional.nll_loss(p, gold_word)\n",
    "\n",
    "# this line might fail\n",
    "optimizer = torch.optim.Adam(mnn.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in epochs:\n",
    "    loss = 0\n",
    "    for example in examples:\n",
    "        # batch size 1\n",
    "        board, gold_word = example\n",
    "        loss += loss_function(clue_dist(embedding, board), gold_word)\n",
    "    \n",
    "    # backprop!\n",
    "    loss.backward()\n",
    "    \n",
    "    print(neural_network_1.params)\n",
    "    \n",
    "    # gradient descent given the computed gradients\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for debugging\n",
    "\n",
    "# Pass in words like ['cat', 'dog', 'fish']\n",
    "def generate_board_from_words(words):\n",
    "    return np.array([word_to_idx[word] for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "board = generate_single_board(codenames_words_idxs)\n",
    "print([idx_to_word[idx] for idx in board])\n",
    "\n",
    "print('Cosine Similarity Scoring Clues:')\n",
    "# Cosine Clues\n",
    "clues = return_top_clues(bert_embedding, board, 'cosine', 0.000024)\n",
    "print(clues[:20])\n",
    "\n",
    "print('Euclidean Scoring Clues:')\n",
    "# Euclidean Clues\n",
    "clues = return_top_clues(bert_embedding, board, 'euclidean', 0.001)\n",
    "print(clues[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_clue_generator(n): \n",
    "    for i in range(n):  \n",
    "        board = generate_single_board(codenames_words_idxs)\n",
    "        clues = return_top_clues(bert_embedding, board, 'euclidean')\n",
    "        if len(clues) > 10:\n",
    "            print([idx_to_word[idx] for idx in board])\n",
    "            print(clues[:20])\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_clue_generator(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_single_board(codenames_words):\n",
    "    #randomly selects three words from the codenames board\n",
    "    board = sample(codenames_words, 3)\n",
    "    print('The board we will be finding clues for is', board)\n",
    "    return board\n",
    "    \n",
    "def clue_score(embeddings, board, clue):\n",
    "    #generates a score for a single clue given the board\n",
    "    if clue in board: #this makes the score for one of the board words really bad\n",
    "        return -10**6 \n",
    "    score = 0\n",
    "    for board_word in board:\n",
    "        #score += spatial.distance.cosine(embeddings[clue], embeddings[board_word])#1-cosine scoring function\n",
    "        score += np.linalg.norm(embeddings[clue]-embeddings[board_word])\n",
    "    return score\n",
    "    \n",
    "def clue_dict(embeddings, board):\n",
    "    #a dictionary for a given board, where the key is the potential clue and the value is the clue score\n",
    "    score_dict = dict()\n",
    "    for word in embeddings:\n",
    "        score_dict[word] = clue_score(embeddings, board, word)\n",
    "    return score_dict\n",
    "\n",
    "def extract_top_clues(score_dict):\n",
    "    sorted_score_dict = sorted(score_dict.items(), key=lambda x:x[1])\n",
    "    top_ten_clues = [sorted_score_dict[i][0] for i in range(3,3+10)]\n",
    "    print('Top ten clues are', top_ten_clues)\n",
    "\n",
    "board = generate_single_board(codenames_words)\n",
    "score_dict_bert = clue_dict(bert_dict, board)\n",
    "score_dict_glove = clue_dict(glove_dict, board)\n",
    "sorted_score_dict = extract_top_clues(score_dict_bert)\n",
    "sorted_score_dict = extract_top_clues(score_dict_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.linalg.norm(bert_dict['belt']-bert_dict['kangaroo']))\n",
    "print(np.linalg.norm(bert_dict['belt']-bert_dict['hotel']))\n",
    "print(np.linalg.norm(bert_dict['kangaroo']-bert_dict['hotel']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spatial.distance.cosine(glove_dict['belt'],glove_dict['kangaroo']))\n",
    "print(spatial.distance.cosine(glove_dict['belt'],glove_dict['hotel']))\n",
    "print(spatial.distance.cosine(glove_dict['kangaroo'],glove_dict['hotel']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
