{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import spatial\n",
    "from random import sample\n",
    "import copy\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import encoding dictionaries. i.e. bert_dict['cat'] = [0.5, -.4, 0.001,...]\n",
    "bert_dict = np.load('bert.npy', allow_pickle = True); bert_dict = bert_dict[()]\n",
    "glove_dict = np.load('glove.npy', allow_pickle = True); glove_dict = glove_dict[()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import list of codenames words\n",
    "codenames_words = []\n",
    "with open(\"./codenames_words.txt\", 'r') as f:\n",
    "    for line in f:\n",
    "        codenames_words.append(line.strip())\n",
    "        \n",
    "# Remove codenames words not in embeddings\n",
    "def removeInvalidCodenames(embeddings, codenames_words):\n",
    "    removed_words = []\n",
    "    for item in codenames_words:\n",
    "        if item not in embeddings:\n",
    "            removed_words.append(item)\n",
    "            codenames_words.remove(item)\n",
    "    #print('Words removed from codenames are', removed_words)\n",
    "    return codenames_words\n",
    "\n",
    "codenames_words_glove = removeInvalidCodenames(glove_dict, codenames_words)\n",
    "codenames_words = removeInvalidCodenames(bert_dict, codenames_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn BERT into indexable embeddings\n",
    "all_bert_words = list(bert_dict.keys())\n",
    "word_to_idx = dict(zip(all_bert_words, range(len(all_bert_words))))\n",
    "idx_to_word = dict(zip(range(len(all_bert_words)), all_bert_words))\n",
    "\n",
    "# Get indexes for codenames words\n",
    "codenames_words_idxs = np.array([word_to_idx[word] for word in codenames_words])\n",
    "\n",
    "# Get BERT embeddings as giant matrix\n",
    "bert_embedding = [bert_dict[word] for word in all_bert_words]\n",
    "bert_embedding = np.vstack(bert_embedding) # shape = (42000, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "# Returns list of random indexes from indexes of codenames words\n",
    "def generate_single_board(codenames_words_idxs): \n",
    "    return np.random.choice(codenames_words_idxs, 3)\n",
    "\n",
    "# Calculates prob. distribution (using softmax) over entire vocabulary for a single board\n",
    "def clue_dist_for_one_board(embedding, board, scoring = 'cosine', temperature = 1):\n",
    "    '''\n",
    "    embedding: V x [embedding_size = 768]\n",
    "    board: (3,) [indices of good words in all_bert_words]\n",
    "    scoring: either cosine or euclidean\n",
    "    temperature: sampling smoothing over prob. distribution\n",
    "    '''\n",
    "    \n",
    "    V, K = embedding.shape\n",
    "    board_embedding = embedding[board] # shape = (3 x 768)\n",
    "    \n",
    "    # Calculate score for each word in vocabulary using the specified scoring function\n",
    "    if scoring == 'euclidean': \n",
    "        score = -((board_embedding.reshape(1, 3, K) - embedding.reshape(V, 1, K)) ** 2).sum(2).sum(1) # shape V\n",
    "    else: # cosine\n",
    "        score = (np.matmul(board_embedding, embedding.T) / \n",
    "                 (np.matmul(np.linalg.norm(board_embedding, axis=1, keepdims=True), \n",
    "                            np.linalg.norm(embedding, axis=1, keepdims=True).T))).sum(0)\n",
    "    \n",
    "    # Penalize scores for good words\n",
    "    score[board] -= 1e6\n",
    "    \n",
    "    return softmax(score / temperature)\n",
    "\n",
    "# Returns the top clues for a certain board based on the probability distribution over the vocabulary\n",
    "def return_top_clues(embedding_matrix, board, scoring, p_value): \n",
    "    # probability distribution for one board\n",
    "    p = clue_dist_for_one_board(embedding_matrix, board, scoring, 15)\n",
    "    return [idx_to_word[word] for word in np.arange(p.shape[0])[p > p_value]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clue_dist_for_one_board_pt(embedding, board, scoring='cosine', temperature=1):\n",
    "    \"\"\"\n",
    "    Assume for now that the inputs are numpy matrices:\n",
    "    embedding: V x K matrix of floats\n",
    "    board: (3,) matrix of ints\n",
    "    \"\"\"\n",
    "    embedding_pt = torch.tensor(embedding, requires_grad=False)\n",
    "    board_pt = torch.tensor(board, requires_grad=False)\n",
    "    V, K = embedding.shape\n",
    "    \n",
    "    # let's try it \n",
    "    board_embedding_pt = embedding_pt[board_pt].view(1, 3 * K) # 3 x 768\n",
    "    board_embedding_big = board_embedding_pt.expand(V, 3 * K)\n",
    "    all_input = torch.concat([embedding_pt, board_embedding_big], dim=1) # (V x 3072)\n",
    "    \n",
    "    # deeplearning ML whoooo\n",
    "    neural_network_1 = nn.Linear(3072, 100)\n",
    "    neural_network_2 = nn.Linear(100, 1)\n",
    "    \n",
    "    output = neural_network_2(neural_network_1(all_input)) # V x 1\n",
    "    return torch.softmax(output / temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Linear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "board = generate_single_board(codenames_words_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNeuralNetwork(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(MyNeuralNetwork, self).__init__()\n",
    "        self.neural_network_1 = nn.Linear(3072, hidden_size)\n",
    "        self.neural_network_2 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return nn.Tanh(self.neural_network_2(nn.Tanh(self.neural_network_1(x))))5678"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(5000, 3072)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(mnn(x) == mnn.forward(x)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000, 1])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnn = MyNeuralNetwork(50)\n",
    "mnn(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-16-5c15806c50c8>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-16-5c15806c50c8>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    def clue_dist_...\u001b[0m\n\u001b[1;37m                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "mnn = MyNeuralNetwork(50)\n",
    "\n",
    "def clue_dist_...\n",
    "    \n",
    "    output = mnn(all_input)\n",
    "    \n",
    "    return torch.softmax()\n",
    "\n",
    "def loss_function(p, gold_word):\n",
    "    return torch.nn.functional.nll_loss(p, gold_word)\n",
    "\n",
    "# this line might fail\n",
    "optimizer = torch.optim.Adam(mnn.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in epochs:\n",
    "    loss = 0\n",
    "    for example in examples:\n",
    "        # batch size 1\n",
    "        board, gold_word = example\n",
    "        loss += loss_function(clue_dist(embedding, board), gold_word)\n",
    "    \n",
    "    # backprop!\n",
    "    loss.backward()\n",
    "    \n",
    "    print(neural_network_1.params)\n",
    "    \n",
    "    # gradient descent given the computed gradients\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for debugging\n",
    "\n",
    "# Pass in words like ['cat', 'dog', 'fish']\n",
    "def generate_board_from_words(words):\n",
    "    return np.array([word_to_idx[word] for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eye', 'dinosaur', 'ruler']\n",
      "Cosine Similarity Scoring Clues:\n",
      "['expert', 'aware', 'native', 'garage', 'circle', 'liquid', 'monster', 'dealer', 'tracking', 'dean', 'sensor', 'lounge', 'killer', 'basket', 'galaxy', 'talented', 'flavor', 'facial', 'citizen', 'container']\n",
      "Euclidean Scoring Clues:\n",
      "['alien', 'wizard', 'banana', 'vegetarian', 'clone', 'observer', 'sphere', 'abnormal', 'flagship', 'squirrel', 'surfer', 'sidekick', 'sorcerer', 'primate', 'settler']\n"
     ]
    }
   ],
   "source": [
    "board = generate_single_board(codenames_words_idxs)\n",
    "print([idx_to_word[idx] for idx in board])\n",
    "\n",
    "print('Cosine Similarity Scoring Clues:')\n",
    "# Cosine Clues\n",
    "clues = return_top_clues(bert_embedding, board, 'cosine', 0.000024)\n",
    "print(clues[:20])\n",
    "\n",
    "print('Euclidean Scoring Clues:')\n",
    "# Euclidean Clues\n",
    "clues = return_top_clues(bert_embedding, board, 'euclidean', 0.001)\n",
    "print(clues[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_clue_generator(n): \n",
    "    for i in range(n):  \n",
    "        board = generate_single_board(codenames_words_idxs)\n",
    "        clues = return_top_clues(bert_embedding, board, 'euclidean')\n",
    "        if len(clues) > 10:\n",
    "            print([idx_to_word[idx] for idx in board])\n",
    "            print(clues[:20])\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "return_top_clues() missing 1 required positional argument: 'p_value'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-451d69807e58>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbert_clue_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-1ddbd78806b2>\u001b[0m in \u001b[0;36mbert_clue_generator\u001b[1;34m(n)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mboard\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_single_board\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodenames_words_idxs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mclues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreturn_top_clues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbert_embedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mboard\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'euclidean'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclues\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx_to_word\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mboard\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: return_top_clues() missing 1 required positional argument: 'p_value'"
     ]
    }
   ],
   "source": [
    "bert_clue_generator(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The board we will be finding clues for is ['rome', 'fly', 'opera']\n",
      "Top ten clues are ['operas', 'mermaid', 'ballet', 'underworld', 'metropolis', 'musica', 'm√∫sica', 'bath', 'kite', 'gloria']\n",
      "Top ten clues are ['hence', 'latter', 'brought', 'aka', 'meant', 'forget', 'mention', 'instance', 'fro', 'wonder']\n"
     ]
    }
   ],
   "source": [
    "def generate_single_board(codenames_words):\n",
    "    #randomly selects three words from the codenames board\n",
    "    board = sample(codenames_words, 3)\n",
    "    print('The board we will be finding clues for is', board)\n",
    "    return board\n",
    "    \n",
    "def clue_score(embeddings, board, clue):\n",
    "    #generates a score for a single clue given the board\n",
    "    if clue in board: #this makes the score for one of the board words really bad\n",
    "        return -10**6 \n",
    "    score = 0\n",
    "    for board_word in board:\n",
    "        #score += spatial.distance.cosine(embeddings[clue], embeddings[board_word])#1-cosine scoring function\n",
    "        score += np.linalg.norm(embeddings[clue]-embeddings[board_word])\n",
    "    return score\n",
    "    \n",
    "def clue_dict(embeddings, board):\n",
    "    #a dictionary for a given board, where the key is the potential clue and the value is the clue score\n",
    "    score_dict = dict()\n",
    "    for word in embeddings:\n",
    "        score_dict[word] = clue_score(embeddings, board, word)\n",
    "    return score_dict\n",
    "\n",
    "def extract_top_clues(score_dict):\n",
    "    sorted_score_dict = sorted(score_dict.items(), key=lambda x:x[1])\n",
    "    top_ten_clues = [sorted_score_dict[i][0] for i in range(3,3+10)]\n",
    "    print('Top ten clues are', top_ten_clues)\n",
    "\n",
    "board = generate_single_board(codenames_words)\n",
    "score_dict_bert = clue_dict(bert_dict, board)\n",
    "score_dict_glove = clue_dict(glove_dict, board)\n",
    "sorted_score_dict = extract_top_clues(score_dict_bert)\n",
    "sorted_score_dict = extract_top_clues(score_dict_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.linalg.norm(bert_dict['belt']-bert_dict['kangaroo']))\n",
    "print(np.linalg.norm(bert_dict['belt']-bert_dict['hotel']))\n",
    "print(np.linalg.norm(bert_dict['kangaroo']-bert_dict['hotel']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spatial.distance.cosine(glove_dict['belt'],glove_dict['kangaroo']))\n",
    "print(spatial.distance.cosine(glove_dict['belt'],glove_dict['hotel']))\n",
    "print(spatial.distance.cosine(glove_dict['kangaroo'],glove_dict['hotel']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
