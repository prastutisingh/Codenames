{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not sure if we need all of this...\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import spatial\n",
    "from random import sample\n",
    "import copy\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import encoding dictionaries. i.e. bert_dict['cat'] = [0.5, -.4, 0.001,...]\n",
    "bert_dict = np.load('bert.npy', allow_pickle = True); bert_dict = bert_dict[()]\n",
    "glove_dict = np.load('glove.npy', allow_pickle = True); glove_dict = glove_dict[()]\n",
    "\n",
    "# Part of speech tagging (already done on GloVe embeddings)\n",
    "bert_dict_copy = bert_dict.copy()\n",
    "\n",
    "for key in bert_dict_copy:  \n",
    "    tag = nltk.pos_tag([key.strip()])[0][1]\n",
    "    if tag not in ['NN', 'NNP']: \n",
    "        del bert_dict[key]\n",
    "\n",
    "# Turn BERT into indexable embeddings\n",
    "all_bert_words = list(bert_dict.keys())\n",
    "word_to_idx = dict(zip(all_bert_words, range(len(all_bert_words))))\n",
    "idx_to_word = dict(zip(range(len(all_bert_words)), all_bert_words))\n",
    "\n",
    "# Get BERT embeddings as giant matrix\n",
    "bert_embedding = [bert_dict[word] for word in all_bert_words]\n",
    "bert_embedding = np.vstack(bert_embedding) # shape = (30000, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from txt file\n",
    "def read_data(examples, data): \n",
    "    '''\n",
    "    Takes in dataset from examples and data (make sure they're for the same data!!!) and appends each example to\n",
    "    a matrix. \n",
    "    For example, \n",
    "        board = 'cat', 'dog' (from examples)\n",
    "        gold_word = 'pet' (from data)\n",
    "    Will be appended as: \n",
    "        [[word_to_idx['cat], word_to_idx['dog']], [word_to_idx['dog']]]\n",
    "    '''\n",
    "    data_matrix = []\n",
    "    with open(examples) as examples, open(data) as data:\n",
    "        for line1, line2 in zip(examples, data):\n",
    "            input_line_1 = line1.split('.')\n",
    "            good_words = input_line_1[1].strip('\\n').split(',')[:2]\n",
    "            \n",
    "            input_line_2 = line2.split('.')\n",
    "            gold_word = input_line_2[1].strip()\n",
    "            \n",
    "            if input_line_1[0] == input_line_2[0]: \n",
    "                board = [word_to_idx[word.strip()] for word in good_words]\n",
    "                if gold_word == 'No good clues': \n",
    "                    continue\n",
    "            \n",
    "                try: \n",
    "                    data_matrix.append([board, [word_to_idx[gold_word]]])\n",
    "                except: \n",
    "                    continue\n",
    "            else: \n",
    "                print(input_line_1[0])\n",
    "                print(input_line_2[0])\n",
    "                print('Uh oh, misaligment!')\n",
    "                break\n",
    "            \n",
    "    return data_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        # Layer 1\n",
    "        self.neural_network_1 = nn.Linear(2304, hidden_size)\n",
    "        \n",
    "        # Layer 2\n",
    "        self.neural_network_2 = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "        # Tanh\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    # x is of size V x 1536 \n",
    "    def forward(self, x):  \n",
    "        # First layer\n",
    "        s = self.neural_network_1(x)\n",
    "        \n",
    "        # First Tanh\n",
    "        s = self.tanh(s)\n",
    "        \n",
    "        # Second layer\n",
    "        s = self.neural_network_2(s)\n",
    "        \n",
    "        return s\n",
    "    \n",
    "    # Accuracy\n",
    "    def accuracy(out, labels):\n",
    "        outputs = np.argmax(out, axis=1)\n",
    "        return np.sum(outputs==labels)/float(labels.size)\n",
    "\n",
    "def loss_function(p_dist, gold_word):\n",
    "    return torch.nn.CrossEntropyLoss()(p_dist, gold_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data, optimizer, loss_function, params): \n",
    "    epochs = params['num_epochs']\n",
    "    embedding = params['embedding']\n",
    "    \n",
    "    embedding_pt = torch.tensor(embedding, requires_grad=False)\n",
    "    V, K = embedding.shape\n",
    "    \n",
    "    all_loss = []\n",
    "    \n",
    "    for epoch in range(epochs): \n",
    "        print('Epoch: {}'.format(epoch))\n",
    "        \n",
    "        # Set model to train\n",
    "        model.train()\n",
    "        \n",
    "        # Initialize loss\n",
    "        loss = 0\n",
    "        \n",
    "        for example in train_data:\n",
    "            board, gold_word = example\n",
    "            \n",
    "            board_pt = torch.tensor(board, requires_grad=False)\n",
    "            gold_word_pt = torch.tensor(gold_word, requires_grad=False)\n",
    "    \n",
    "            board_embedding_pt = embedding_pt[board_pt].view(1, 2 * K) # 2 x 768\n",
    "            board_embedding_big = board_embedding_pt.expand(V, 2 * K) # V x (2 X 768)\n",
    "            \n",
    "            # Add embedding for each vocab word to board_embedding_big\n",
    "            all_input = torch.cat([embedding_pt, board_embedding_big], dim=1) # (V x 1536)\n",
    "            \n",
    "            output = model(all_input)\n",
    "            output_T = torch.transpose(output, 0, 1)\n",
    "            \n",
    "            loss += loss_function(output_T, gold_word_pt)\n",
    "        \n",
    "        all_loss.append(loss)\n",
    "        \n",
    "        # Clear previous gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Backprop\n",
    "        loss.backward()\n",
    "        \n",
    "        # Perform update using \n",
    "        optimizer.step()\n",
    "    \n",
    "    plt.plot(all_loss)\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_data, loss_fn, params, idx_to_word): \n",
    "    # This function takes in a model, validation training set, loss function, and params\n",
    "    # And will return the fraction of examples where\n",
    "    # The truth clue (i.e. human chosen) is within the top 10 that the model spits out\n",
    "    \n",
    "    model.eval() #Set model to evaluation mode\n",
    "\n",
    "    count = 0\n",
    "    \n",
    "    embedding_pt = torch.tensor(params['embedding'], requires_grad=False)\n",
    "\n",
    "    V, K = params['embedding'].shape #(27296, 768)\n",
    "    \n",
    "    for example in val_data: \n",
    "        board, gold_word = example; #board = [X, Y], gold_word = [Z]\n",
    "           \n",
    "        board_pt = torch.tensor(board, requires_grad=False) #tensorfy the board, and say its not a variable that can be updated with backprop\n",
    "\n",
    "        board_embedding_pt = embedding_pt[board_pt].view(1, 2 * K) # torch.Size([1, 1536])\n",
    "        board_embedding_big = board_embedding_pt.expand(V, 2 * K) # torch.Size([27296, 1536])\n",
    "\n",
    "        # Add embedding for each vocab word to board_embedding_big\n",
    "        all_input = torch.cat([embedding_pt, board_embedding_big], dim=1) # torch.Size([27296, 2304])\n",
    "        \n",
    "        output = model(all_input) #torch.Size([27296, 1]), this should be a score for each potential clue\n",
    "        \n",
    "        #This prints the words of the board and the ground truth\n",
    "        #print('The board is', [idx_to_word[int(board_word)] for board_word in board_pt], 'and the ground truth chosen word is', idx_to_word[gold_word[0]])\n",
    "        \n",
    "        output = torch.transpose(output, 0, 1) #need to take the transpose before doing top k\n",
    "        scores, indices = output.topk(100) #top k gives you the scores and indices of the top k scorring elements\n",
    "        clue_words = [idx_to_word[int(index)] for index in indices[0]] #converts indices into the words\n",
    "        \n",
    "        if idx_to_word[gold_word[0]] in clue_words:\n",
    "            count += 1\n",
    "\n",
    "        return (count/len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "For epoch 0 training accuracy is 0.0 and test accuracy is 0.0\n",
      "Epoch: 1\n",
      "For epoch 1 training accuracy is 0.0 and test accuracy is 0.0\n",
      "Epoch: 2\n",
      "For epoch 2 training accuracy is 0.0 and test accuracy is 0.0\n",
      "Epoch: 3\n",
      "For epoch 3 training accuracy is 0.04 and test accuracy is 0.0\n",
      "Epoch: 4\n",
      "For epoch 4 training accuracy is 0.04 and test accuracy is 0.0\n",
      "Epoch: 5\n",
      "For epoch 5 training accuracy is 0.04 and test accuracy is 0.0\n",
      "Epoch: 6\n",
      "For epoch 6 training accuracy is 0.04 and test accuracy is 0.0\n",
      "Epoch: 7\n",
      "For epoch 7 training accuracy is 0.04 and test accuracy is 0.0\n",
      "Epoch: 8\n",
      "For epoch 8 training accuracy is 0.04 and test accuracy is 0.0\n",
      "Epoch: 9\n",
      "For epoch 9 training accuracy is 0.04 and test accuracy is 0.0\n",
      "Epoch: 10\n",
      "For epoch 10 training accuracy is 0.04 and test accuracy is 0.0\n",
      "Epoch: 11\n",
      "For epoch 11 training accuracy is 0.04 and test accuracy is 0.0\n",
      "Epoch: 12\n",
      "For epoch 12 training accuracy is 0.04 and test accuracy is 0.0\n",
      "Epoch: 13\n",
      "For epoch 13 training accuracy is 0.04 and test accuracy is 0.0\n",
      "Epoch: 14\n",
      "For epoch 14 training accuracy is 0.04 and test accuracy is 0.0\n",
      "Epoch: 15\n",
      "For epoch 15 training accuracy is 0.04 and test accuracy is 0.0\n",
      "Epoch: 16\n",
      "For epoch 16 training accuracy is 0.04 and test accuracy is 0.0\n",
      "Epoch: 17\n",
      "For epoch 17 training accuracy is 0.04 and test accuracy is 0.0\n",
      "Epoch: 18\n",
      "For epoch 18 training accuracy is 0.04 and test accuracy is 0.0\n",
      "Epoch: 19\n",
      "For epoch 19 training accuracy is 0.04 and test accuracy is 0.0\n",
      "Epoch: 20\n",
      "For epoch 20 training accuracy is 0.04 and test accuracy is 0.0\n",
      "Epoch: 21\n",
      "For epoch 21 training accuracy is 0.04 and test accuracy is 0.0\n",
      "Epoch: 22\n",
      "For epoch 22 training accuracy is 0.04 and test accuracy is 0.0\n",
      "Epoch: 23\n",
      "For epoch 23 training accuracy is 0.04 and test accuracy is 0.0\n",
      "Epoch: 24\n",
      "For epoch 24 training accuracy is 0.04 and test accuracy is 0.0\n",
      "Epoch: 25\n",
      "For epoch 25 training accuracy is 0.04 and test accuracy is 0.0\n",
      "Epoch: 26\n",
      "For epoch 26 training accuracy is 0.04 and test accuracy is 0.0\n",
      "Epoch: 27\n",
      "For epoch 27 training accuracy is 0.04 and test accuracy is 0.0\n",
      "Epoch: 28\n",
      "For epoch 28 training accuracy is 0.04 and test accuracy is 0.0\n",
      "Epoch: 29\n",
      "For epoch 29 training accuracy is 0.04 and test accuracy is 0.0\n",
      "Epoch: 30\n",
      "For epoch 30 training accuracy is 0.04 and test accuracy is 0.0\n",
      "Epoch: 31\n",
      "For epoch 31 training accuracy is 0.04 and test accuracy is 0.0\n",
      "Epoch: 32\n",
      "For epoch 32 training accuracy is 0.04 and test accuracy is 0.0\n",
      "Epoch: 33\n",
      "For epoch 33 training accuracy is 0.04 and test accuracy is 0.0\n",
      "Epoch: 34\n",
      "For epoch 34 training accuracy is 0.04 and test accuracy is 0.0\n",
      "Epoch: 35\n",
      "For epoch 35 training accuracy is 0.04 and test accuracy is 0.0\n",
      "Epoch: 36\n",
      "For epoch 36 training accuracy is 0.04 and test accuracy is 0.0\n",
      "Epoch: 37\n",
      "For epoch 37 training accuracy is 0.04 and test accuracy is 0.0\n",
      "Epoch: 38\n",
      "For epoch 38 training accuracy is 0.04 and test accuracy is 0.0\n",
      "Epoch: 39\n",
      "For epoch 39 training accuracy is 0.04 and test accuracy is 0.0\n",
      "Epoch: 40\n",
      "For epoch 40 training accuracy is 0.04 and test accuracy is 0.0\n",
      "Epoch: 41\n",
      "For epoch 41 training accuracy is 0.04 and test accuracy is 0.0\n",
      "Epoch: 42\n",
      "For epoch 42 training accuracy is 0.04 and test accuracy is 0.0\n",
      "Epoch: 43\n",
      "For epoch 43 training accuracy is 0.04 and test accuracy is 0.0\n",
      "Epoch: 44\n",
      "For epoch 44 training accuracy is 0.04 and test accuracy is 0.0\n",
      "Epoch: 45\n",
      "For epoch 45 training accuracy is 0.04 and test accuracy is 0.0\n",
      "Epoch: 46\n",
      "For epoch 46 training accuracy is 0.04 and test accuracy is 0.0\n",
      "Epoch: 47\n",
      "For epoch 47 training accuracy is 0.04 and test accuracy is 0.0\n",
      "Epoch: 48\n",
      "For epoch 48 training accuracy is 0.04 and test accuracy is 0.0\n",
      "Epoch: 49\n",
      "For epoch 49 training accuracy is 0.04 and test accuracy is 0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Random seed for reproducible experiments\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Load data\n",
    "data_matrix = read_data('examples_all.txt', 'data_all.txt')\n",
    "train_data, val_data = random_split(data_matrix, [25, 223])\n",
    "\n",
    "# Includes things like # of epochs, etc.\n",
    "params = {\n",
    "    'num_epochs' : 50,\n",
    "    'embedding' : bert_embedding\n",
    "}\n",
    "\n",
    "model = NeuralNetwork(hidden_size = 50)\n",
    "\n",
    "# Choice of optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
    "\n",
    "train_and_evaluate(model, train_data, val_data, optimizer, loss_function, params, idx_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, train_data, val_data, optimizer, loss_function, params, idx_to_word): \n",
    "    epochs = params['num_epochs']\n",
    "    embedding = params['embedding']\n",
    "    \n",
    "    embedding_pt = torch.tensor(embedding, requires_grad=False)\n",
    "    V, K = embedding.shape\n",
    "    \n",
    "    all_loss = []\n",
    "    \n",
    "    for epoch in range(epochs): \n",
    "        print('Epoch: {}'.format(epoch))\n",
    "        \n",
    "        # Set model to train\n",
    "        model.train()\n",
    "        \n",
    "        # Initialize loss\n",
    "        loss = 0\n",
    "        \n",
    "        for example in train_data:\n",
    "            board, gold_word = example\n",
    "            \n",
    "            board_pt = torch.tensor(board, requires_grad=False)\n",
    "            gold_word_pt = torch.tensor(gold_word, requires_grad=False)\n",
    "    \n",
    "            board_embedding_pt = embedding_pt[board_pt].view(1, 2 * K) # 2 x 768\n",
    "            board_embedding_big = board_embedding_pt.expand(V, 2 * K) # V x (2 X 768)\n",
    "            \n",
    "            # Add embedding for each vocab word to board_embedding_big\n",
    "            all_input = torch.cat([embedding_pt, board_embedding_big], dim=1) # (V x 1536)\n",
    "            \n",
    "            output = model(all_input)\n",
    "            output_T = torch.transpose(output, 0, 1)\n",
    "            \n",
    "            loss += loss_function(output_T, gold_word_pt)\n",
    "        \n",
    "        all_loss.append(loss)\n",
    "        \n",
    "        # Clear previous gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Backprop\n",
    "        loss.backward()\n",
    "        \n",
    "        # Perform update using \n",
    "        optimizer.step()\n",
    "        \n",
    "        val_acc = evaluate(model, val_data, loss_function, params, idx_to_word)\n",
    "        train_acc = evaluate(model, train_data, loss_function, params, idx_to_word)\n",
    "        print('For epoch', epoch, 'training accuracy is', train_acc, 'and test accuracy is', val_acc)\n",
    "    \n",
    "    plt.plot(all_loss)\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
