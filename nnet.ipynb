{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        # Do we want an embedding layer??\n",
    "        # self.embedding = ...\n",
    "        \n",
    "        # Layer 1\n",
    "        self.neural_network_1 = nn.Linear(3072, hidden_size)\n",
    "        \n",
    "        # Layer 2\n",
    "        self.neural_network_2 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, s):                                \n",
    "        # apply the embedding layer that maps each token to its embedding\n",
    "        # dim: batch_size x seq_len x embedding_dim\n",
    "        # s = self.embedding(s)\n",
    "        \n",
    "        # First layer\n",
    "        s = self.neural_network_1(x)\n",
    "        \n",
    "        # First Tanh\n",
    "        s = nn.Tanh(s)\n",
    "        \n",
    "        # Second layer\n",
    "        s = self.neural_network_2(s)\n",
    "        \n",
    "        # Second Tanh\n",
    "        s = nn.Tanh(s)\n",
    "        \n",
    "        return torch.softmax(s, dim=1)\n",
    "\n",
    "\n",
    "def loss_function(p_dist, gold_word):\n",
    "    return torch.nn.functional.nll_loss(p_dist, gold_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clue_dist(): \n",
    "    \n",
    "\n",
    "def train(model, train_data, val_data, optimizer, loss_function, params, model_name): \n",
    "    # What does this line do?\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in epochs: \n",
    "        loss = 0\n",
    "        \n",
    "        for example in examples:\n",
    "            # batch size 1\n",
    "            board, gold_word = example\n",
    "            \n",
    "            embedding_pt = torch.tensor(embedding, requires_grad=False)\n",
    "            board_pt = torch.tensor(board, requires_grad=False)\n",
    "            V, K = embedding.shape\n",
    "    \n",
    "            board_embedding_pt = embedding_pt[board_pt].view(1, 3 * K) # 3 x 768\n",
    "            board_embedding_big = board_embedding_pt.expand(V, 3 * K)\n",
    "            all_input = torch.concat([embedding_pt, board_embedding_big], dim=1) # (V x 3072)\n",
    "            \n",
    "            output = model(all_input)\n",
    "            \n",
    "            loss += loss_function(output, gold_word)\n",
    "        \n",
    "        # Backprop\n",
    "        loss.backward()\n",
    "        \n",
    "        # Log data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random seed for reproducible experiments\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Load data\n",
    "train_data, val_data = ...\n",
    "\n",
    "model = NeuralNetwork(hidden_size = 50)\n",
    "optimizer = torch.optim.Adam(mnn.parameters(), lr=0.001)\n",
    "\n",
    "# Includes things like # of epochs, train_size, val_size, etc...\n",
    "params = ...\n",
    "\n",
    "train(model, train_data, val_data, optimizer, loss_function, params, model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
