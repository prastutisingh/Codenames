{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not sure if we need all of this...\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import spatial\n",
    "from random import sample\n",
    "import copy\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import encoding dictionaries. i.e. bert_dict['cat'] = [0.5, -.4, 0.001,...]\n",
    "bert_dict = np.load('bert.npy', allow_pickle = True); bert_dict = bert_dict[()]\n",
    "glove_dict = np.load('glove.npy', allow_pickle = True); glove_dict = glove_dict[()]\n",
    "\n",
    "# Part of speech tagging (already done on GloVe embeddings)\n",
    "bert_dict_copy = bert_dict.copy()\n",
    "\n",
    "for key in bert_dict_copy:  \n",
    "    tag = nltk.pos_tag([key.strip()])[0][1]\n",
    "    if tag not in ['NN', 'NNP']: \n",
    "        del bert_dict[key]\n",
    "\n",
    "# Turn BERT into indexable embeddings\n",
    "all_bert_words = list(bert_dict.keys())\n",
    "word_to_idx = dict(zip(all_bert_words, range(len(all_bert_words))))\n",
    "idx_to_word = dict(zip(range(len(all_bert_words)), all_bert_words))\n",
    "\n",
    "# Get BERT embeddings as giant matrix\n",
    "bert_embedding = [bert_dict[word] for word in all_bert_words]\n",
    "bert_embedding = np.vstack(bert_embedding) # shape = (30000, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from txt file\n",
    "def read_data(examples, data): \n",
    "    '''\n",
    "    Takes in dataset from examples and data (make sure they're for the same data!!!) and appends each example to\n",
    "    a matrix. \n",
    "    For example, \n",
    "        board = 'cat', 'dog' (from examples)\n",
    "        gold_word = 'pet' (from data)\n",
    "    Will be appended as: \n",
    "        [[word_to_idx['cat], word_to_idx['dog']], [word_to_idx['dog']]]\n",
    "    '''\n",
    "    data_matrix = []\n",
    "    with open(examples) as examples, open(data) as data:\n",
    "        for line1, line2 in zip(examples, data):\n",
    "            input_line_1 = line1.split('.')\n",
    "            good_words = input_line_1[1].strip('\\n').split(',')[:2]\n",
    "            \n",
    "            input_line_2 = line2.split('.')\n",
    "            gold_word = input_line_2[1].strip()\n",
    "            \n",
    "            if input_line_1[0] == input_line_2[0]: \n",
    "                board = [word_to_idx[word.strip()] for word in good_words]\n",
    "                if gold_word == 'No good clues': \n",
    "                    continue\n",
    "            \n",
    "                try: \n",
    "                    data_matrix.append([board, [word_to_idx[gold_word]]])\n",
    "                except: \n",
    "                    continue\n",
    "            else: \n",
    "                print(input_line_1[0])\n",
    "                print(input_line_2[0])\n",
    "                print('Uh oh, misaligment!')\n",
    "                break\n",
    "            \n",
    "    return data_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        # Layer 1\n",
    "        self.neural_network_1 = nn.Linear(2304, hidden_size)\n",
    "        \n",
    "        # Layer 2\n",
    "        self.neural_network_2 = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "        # Tanh\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    # x is of size V x 1536 \n",
    "    def forward(self, x):  \n",
    "        # First layer\n",
    "        s = self.neural_network_1(x)\n",
    "        \n",
    "        # First Tanh\n",
    "        s = self.tanh(s)\n",
    "        \n",
    "        # Second layer\n",
    "        s = self.neural_network_2(s)\n",
    "        \n",
    "        return s\n",
    "    \n",
    "    # Accuracy\n",
    "    def accuracy(out, labels):\n",
    "        outputs = np.argmax(out, axis=1)\n",
    "        return np.sum(outputs==labels)/float(labels.size)\n",
    "\n",
    "def loss_function(p_dist, gold_word):\n",
    "    return torch.nn.CrossEntropyLoss()(p_dist, gold_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data, optimizer, loss_function, params): \n",
    "    epochs = params['num_epochs']\n",
    "    embedding = params['embedding']\n",
    "    \n",
    "    embedding_pt = torch.tensor(embedding, requires_grad=False)\n",
    "    V, K = embedding.shape\n",
    "    \n",
    "    all_loss = []\n",
    "    \n",
    "    for epoch in range(epochs): \n",
    "        print('Epoch: {}'.format(epoch))\n",
    "        \n",
    "        # Set model to train\n",
    "        model.train()\n",
    "        \n",
    "        # Initialize loss\n",
    "        loss = 0\n",
    "        \n",
    "        for example in train_data:\n",
    "            board, gold_word = example\n",
    "            \n",
    "            board_pt = torch.tensor(board, requires_grad=False)\n",
    "            gold_word_pt = torch.tensor(gold_word, requires_grad=False)\n",
    "    \n",
    "            board_embedding_pt = embedding_pt[board_pt].view(1, 2 * K) # 2 x 768\n",
    "            board_embedding_big = board_embedding_pt.expand(V, 2 * K) # V x (2 X 768)\n",
    "            \n",
    "            # Add embedding for each vocab word to board_embedding_big\n",
    "            all_input = torch.cat([embedding_pt, board_embedding_big], dim=1) # (V x 1536)\n",
    "            \n",
    "            output = model(all_input)\n",
    "            output_T = torch.transpose(output, 0, 1)\n",
    "            \n",
    "            loss += loss_function(output_T, gold_word_pt)\n",
    "        \n",
    "        all_loss.append(loss)\n",
    "        \n",
    "        # Clear previous gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Backprop\n",
    "        loss.backward()\n",
    "        \n",
    "        # Perform update using \n",
    "        optimizer.step()\n",
    "    \n",
    "    plt.plot(all_loss)\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.show()\n",
    "    \n",
    "def evaluate(model, val_data, loss_fn, params): \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    count = 0\n",
    "    \n",
    "    embedding_pt = torch.tensor(params['embedding'], requires_grad=False)\n",
    "    V, K = params['embedding'].shape\n",
    "    \n",
    "    for example in val_data: \n",
    "        board, gold_word = example\n",
    "            \n",
    "        board_pt = torch.tensor(board, requires_grad=False)\n",
    "\n",
    "        board_embedding_pt = embedding_pt[board_pt].view(1, 2 * K) # 2 x 768\n",
    "        board_embedding_big = board_embedding_pt.expand(V, 2 * K) # V x (2 X 768)\n",
    "\n",
    "        # Add embedding for each vocab word to board_embedding_big\n",
    "        all_input = torch.cat([embedding_pt, board_embedding_big], dim=1) # (V x 1536)\n",
    "        \n",
    "        output = model(all_input)\n",
    "        \n",
    "        idx = torch.argmax(output).tolist()\n",
    "        print(idx)\n",
    "        print(gold_word)\n",
    "        \n",
    "        if idx == gold_word: \n",
    "            count += 1\n",
    "            \n",
    "    return (count/len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Epoch: 9\n",
      "Epoch: 10\n",
      "Epoch: 11\n",
      "Epoch: 12\n",
      "Epoch: 13\n",
      "Epoch: 14\n",
      "Epoch: 15\n",
      "Epoch: 16\n",
      "Epoch: 17\n",
      "Epoch: 18\n",
      "Epoch: 19\n",
      "Epoch: 20\n",
      "Epoch: 21\n",
      "Epoch: 22\n",
      "Epoch: 23\n",
      "Epoch: 24\n",
      "Epoch: 25\n",
      "Epoch: 26\n",
      "Epoch: 27\n",
      "Epoch: 28\n",
      "Epoch: 29\n",
      "Epoch: 30\n",
      "Epoch: 31\n",
      "Epoch: 32\n",
      "Epoch: 33\n",
      "Epoch: 34\n",
      "Epoch: 35\n",
      "Epoch: 36\n",
      "Epoch: 37\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-8e9fb6aacf4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-426d300010b5>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_data, optimizer, loss_function, params)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# Backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# Perform update using\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Random seed for reproducible experiments\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Load data\n",
    "data_matrix = read_data('examples.txt', 'data.txt')\n",
    "train_data, val_data = random_split(data_matrix, [25, 9])\n",
    "\n",
    "# Includes things like # of epochs, etc.\n",
    "params = {\n",
    "    'num_epochs' : 50,\n",
    "    'embedding' : bert_embedding\n",
    "}\n",
    "\n",
    "model = NeuralNetwork(hidden_size = 50)\n",
    "\n",
    "# Choice of optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "train(model, train_data, optimizer, loss_function, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1985\n",
      "[3240]\n",
      "1985\n",
      "[3158]\n",
      "1985\n",
      "[392]\n",
      "1985\n",
      "[793]\n",
      "1985\n",
      "[600]\n",
      "1985\n",
      "[3364]\n",
      "1985\n",
      "[2196]\n",
      "1985\n",
      "[15692]\n",
      "1985\n",
      "[2947]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, val_data, loss_function, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
