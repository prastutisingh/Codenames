{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not sure if we need all of this...\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from random import shuffle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import spatial\n",
    "import copy\n",
    "import nltk\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import encoding dictionaries. i.e. bert_dict['cat'] = [0.5, -.4, 0.001,...]\n",
    "bert_dict = np.load('bert.npy', allow_pickle = True); bert_dict = bert_dict[()]\n",
    "glove_dict = np.load('glove.npy', allow_pickle = True); glove_dict = glove_dict[()]\n",
    "\n",
    "# Part of speech tagging (already done on GloVe embeddings)\n",
    "bert_dict_copy = bert_dict.copy()\n",
    "\n",
    "for key in bert_dict_copy:  \n",
    "    tag = nltk.pos_tag([key.strip()])[0][1]\n",
    "    if tag not in ['NN', 'NNP']: \n",
    "        del bert_dict[key]\n",
    "\n",
    "# Turn BERT into indexable embeddings\n",
    "all_bert_words = list(bert_dict.keys())\n",
    "word_to_idx = dict(zip(all_bert_words, range(len(all_bert_words))))\n",
    "idx_to_word = dict(zip(range(len(all_bert_words)), all_bert_words))\n",
    "\n",
    "# Get BERT embeddings as giant matrix\n",
    "bert_embedding = [bert_dict[word] for word in all_bert_words]\n",
    "bert_embedding = np.vstack(bert_embedding) # shape = (30000, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from txt file\n",
    "def read_data(examples, data): \n",
    "    '''\n",
    "    Takes in dataset from examples and data (make sure they're for the same data!!!) and appends each example to\n",
    "    a matrix. \n",
    "    For example, \n",
    "        board = 'cat', 'dog' (from examples)\n",
    "        gold_word = 'pet' (from data)\n",
    "    Will be appended as: \n",
    "        [[word_to_idx['cat], word_to_idx['dog']], [word_to_idx['pet']]]\n",
    "    '''\n",
    "    data_matrix = []\n",
    "    with open(examples) as examples, open(data) as data:\n",
    "        for line1, line2 in zip(examples, data):\n",
    "            input_line_1 = line1.split('.')\n",
    "            good_words = input_line_1[1].strip('\\n').split(',')[:2]\n",
    "            \n",
    "            input_line_2 = line2.split('.')\n",
    "            gold_word = input_line_2[1].strip()\n",
    "            \n",
    "            if input_line_1[0] == input_line_2[0]: \n",
    "                board = [word_to_idx[word.strip()] for word in good_words]\n",
    "                if gold_word == 'No good clues': \n",
    "                    continue\n",
    "            \n",
    "                try: \n",
    "                    data_matrix.append([board, [word_to_idx[gold_word]]])\n",
    "                except: \n",
    "                    continue\n",
    "            else: \n",
    "                print(input_line_1[0])\n",
    "                print(input_line_2[0])\n",
    "                print('Uh oh, misaligment!')\n",
    "                break\n",
    "            \n",
    "    return data_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        # Layer 1\n",
    "        self.neural_network_1 = nn.Linear(2304, hidden_size)\n",
    "        \n",
    "        # Layer 2\n",
    "        self.neural_network_2 = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "        # Tanh\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    # x is of size V x 1536 \n",
    "    def forward(self, x):  \n",
    "        # First layer\n",
    "        s = self.neural_network_1(x)\n",
    "        \n",
    "        # First Tanh\n",
    "        s = self.tanh(s)\n",
    "        \n",
    "        # Second layer\n",
    "        s = self.neural_network_2(s)\n",
    "        \n",
    "        return s\n",
    "    \n",
    "class NeuralNetwork_with_dropout(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(NeuralNetwork_with_dropout, self).__init__()\n",
    "        \n",
    "        # Linear layer 1\n",
    "        self.neural_network_1 = nn.Linear(2304, hidden_size)\n",
    "        \n",
    "        # Dropout layer 1\n",
    "        self.dropout_1 = nn.Dropout(p=0.5)\n",
    "        \n",
    "        # Layer 2\n",
    "        self.neural_network_2 = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "        # Dropout layer 2\n",
    "        self.dropout_2 = nn.Dropout(p=0.2)\n",
    "        \n",
    "        # Tanh\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    # x is of size V x 1536 \n",
    "    def forward(self, x):  \n",
    "        # First layer\n",
    "        s = self.neural_network_1(x)\n",
    "        \n",
    "        # Dropout with probability 0.5\n",
    "        s = self.dropout_1(s)\n",
    "        \n",
    "        # First Tanh\n",
    "        s = self.tanh(s)\n",
    "        \n",
    "        # Second layer\n",
    "        s = self.neural_network_2(s)\n",
    "        \n",
    "        # Dropout with probability 0.2\n",
    "        s = self.dropout_2(s)\n",
    "        \n",
    "        return s\n",
    "    \n",
    "def loss_function(p_dist, gold_word):\n",
    "    return torch.nn.CrossEntropyLoss()(p_dist, gold_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jupyterplot import ProgressPlot\n",
    "\n",
    "def train(model, train_data, optimizer, loss_function, params): \n",
    "    epochs = params['num_epochs']\n",
    "    embedding = params['embedding']\n",
    "    \n",
    "    embedding_pt = torch.tensor(embedding, requires_grad=False)\n",
    "    V, K = embedding.shape\n",
    "    \n",
    "    all_loss = []\n",
    "    mini_batch_loss = []\n",
    "    pp = ProgressPlot()\n",
    "    \n",
    "    batch_size = V\n",
    "    \n",
    "    for epoch in range(epochs): \n",
    "        print('Epoch: {}'.format(epoch))\n",
    "\n",
    "        # Set model to train\n",
    "        model.train()\n",
    "\n",
    "        ## Prep minibatches of size 4 (currently hard-coded, can be changed later)\n",
    "        #m = len(train_data)\n",
    "        #num_batches = round(m / 4)\n",
    "        \n",
    "        epoch_loss = 0\n",
    "\n",
    "        for example in train_data:\n",
    "            board, gold_word = example\n",
    "\n",
    "            board_pt = torch.tensor(board, requires_grad=False)\n",
    "            gold_word_pt = torch.tensor(gold_word, requires_grad=False)\n",
    "\n",
    "            board_embedding_pt = embedding_pt[board_pt].view(1, 2 * K)  # 2 x 768\n",
    "            board_embedding_big = board_embedding_pt.expand(V, 2 * K)  # V x (2 X 768)\n",
    "\n",
    "            # Add embedding for each vocab word to board_embedding_big\n",
    "            all_input = torch.cat([embedding_pt, board_embedding_big], dim=1)  # (V x 1536)\n",
    "            \n",
    "            # TODO : Handle cases where V % 32 != 0 (OK EUGENE?)\n",
    "            for i in range(all_input.size(0) // batch_size): \n",
    "                output = model(all_input[i*batch_size : (i+1)*batch_size])\n",
    "                output_T = torch.transpose(output, 0, 1)\n",
    "\n",
    "                loss = loss_function(output_T, gold_word_pt[i*batch_size : (i+1)*batch_size])\n",
    "\n",
    "                mini_batch_loss.append(float(np.array([loss.detach().cpu().numpy()])[0]))\n",
    "                pp.update(np.mean(mini_batch_loss[-min(len(mini_batch_loss), 100):]))\n",
    "                \n",
    "                # Append loss from batch to epoch_loss\n",
    "                epoch_loss += loss\n",
    "\n",
    "                # Clear previous gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Backprop\n",
    "                loss.backward()\n",
    "\n",
    "                # Perform update using\n",
    "                optimizer.step()\n",
    "            \n",
    "        all_loss.append(epoch_loss)\n",
    "        print('Epoch loss: {}'.format(epoch_loss))\n",
    "    \n",
    "    pp.finalize()\n",
    "    \n",
    "    plt.plot(all_loss)\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_data, params, idx_to_word, word_to_idx): \n",
    "    # This function takes in a model, validation training set, loss function, and params\n",
    "    # And will return the fraction of examples where\n",
    "    # The truth clue (i.e. human chosen) is within the top 10 that the model spits out\n",
    "    \n",
    "    model.eval() #Set model to evaluation mode\n",
    "\n",
    "    count = 0\n",
    "    \n",
    "    embedding_pt = torch.tensor(params['embedding'], requires_grad=False)\n",
    "\n",
    "    V, K = params['embedding'].shape #(27296, 768)\n",
    "    \n",
    "    for example in val_data: \n",
    "        board, gold_word = example; #board = [X, Y], gold_word = [Z]\n",
    "           \n",
    "        board_pt = torch.tensor(board, requires_grad=False) #tensorfy the board, and say its not a variable that can be updated with backprop\n",
    "\n",
    "        board_embedding_pt = embedding_pt[board_pt].view(1, 2 * K) # torch.Size([1, 1536])\n",
    "        board_embedding_big = board_embedding_pt.expand(V, 2 * K) # torch.Size([27296, 1536])\n",
    "        all_input = torch.cat([embedding_pt, board_embedding_big], dim=1) # torch.Size([27296, 2304])\n",
    "        \n",
    "        output = model(all_input) #torch.Size([27296, 1]), this should be a score for each potential clue\n",
    "        \n",
    "        output = torch.transpose(output, 0, 1) # need to take the transpose before doing top k\n",
    "        scores, indices = output.topk(100) #top k gives you the scores and indices of the top k scoring elements\n",
    "        clue_words = [idx_to_word[int(index)] for index in indices[0]] #converts indices into the words\n",
    "        \n",
    "        if idx_to_word[gold_word[0]] in clue_words:\n",
    "            count += 1\n",
    "\n",
    "    return (count/len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Random seed for reproducible experiments\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Load data\n",
    "data_matrix = read_data('examples_all.txt', 'data_all.txt')\n",
    "# train_data, val_data = random_split(data_matrix, [50, 21])\n",
    "\n",
    "train_data, val_data = train_test_split(data_matrix, test_size=0.8)\n",
    "\n",
    "# Includes things like # of epochs, etc.\n",
    "params = {\n",
    "    'num_epochs' : 10,\n",
    "    'embedding' : bert_embedding\n",
    "}\n",
    "\n",
    "model = NeuralNetwork(hidden_size = 50)\n",
    "\n",
    "# Choice of optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "start_time = time.time()\n",
    "train(model, train_data, optimizer, loss_function, params)\n",
    "print('Time: {}'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "evaluate(model, val_data[0:100], params, idx_to_word, word_to_idx)\n",
    "print('Time: {}'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, train_data, val_data, optimizer, loss_function, params): \n",
    "    epochs = params['num_epochs']\n",
    "    embedding = params['embedding']\n",
    "    \n",
    "    embedding_pt = torch.tensor(embedding, requires_grad=False)\n",
    "    V, K = embedding.shape\n",
    "    \n",
    "    all_loss = []\n",
    "    mini_batch_loss = []\n",
    "    pp = ProgressPlot()\n",
    "\n",
    "    val_accuracies = []\n",
    "    train_accuracies = []\n",
    "    \n",
    "    batch_size = V\n",
    "    \n",
    "    for epoch in range(epochs): \n",
    "        print('Epoch: {}'.format(epoch))\n",
    "        \n",
    "        shuffle(train_data)\n",
    "\n",
    "        # Set model to train\n",
    "        model.train()\n",
    "        \n",
    "        val_acc = evaluate(model, val_data, params, idx_to_word, word_to_idx)\n",
    "        val_accuracies.append(val_acc)\n",
    "        train_acc = evaluate(model, train_data, params, idx_to_word, word_to_idx)\n",
    "        train_accuracies.append(train_acc)\n",
    "        \n",
    "        epoch_loss = 0\n",
    "\n",
    "        for example in train_data:\n",
    "            board, gold_word = example\n",
    "\n",
    "            board_pt = torch.tensor(board, requires_grad=False)\n",
    "            gold_word_pt = torch.tensor(gold_word, requires_grad=False)\n",
    "\n",
    "            board_embedding_pt = embedding_pt[board_pt].view(1, 2 * K)  # 2 x 768\n",
    "            board_embedding_big = board_embedding_pt.expand(V, 2 * K)  # V x (2 X 768)\n",
    "\n",
    "            # Add embedding for each vocab word to board_embedding_big\n",
    "            all_input = torch.cat([embedding_pt, board_embedding_big], dim=1)  # (V x 1536)\n",
    "            \n",
    "            # TODO : Handle cases where V % 32 != 0 (OK EUGENE?)\n",
    "            for i in range(all_input.size(0) // batch_size): \n",
    "                output = model(all_input[i*batch_size : (i+1)*batch_size])\n",
    "                output_T = torch.transpose(output, 0, 1)\n",
    "\n",
    "                loss = loss_function(output_T, gold_word_pt[i*batch_size : (i+1)*batch_size])\n",
    "\n",
    "                mini_batch_loss.append(float(np.array([loss.detach().cpu().numpy()])[0]))\n",
    "                pp.update(np.mean(mini_batch_loss[-min(len(mini_batch_loss), 100):]))\n",
    "                \n",
    "                # Append loss from batch to epoch_loss\n",
    "                epoch_loss += loss\n",
    "\n",
    "                # Clear previous gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Backprop\n",
    "                loss.backward()\n",
    "\n",
    "                # Perform update using\n",
    "                optimizer.step()\n",
    "            \n",
    "        all_loss.append(epoch_loss)\n",
    "        print('Epoch loss: {}'.format(epoch_loss))\n",
    "        \n",
    "    val_acc = evaluate(model, val_data, params, idx_to_word, word_to_idx)\n",
    "    val_accuracies.append(val_acc)\n",
    "    train_acc = evaluate(model, train_data, params, idx_to_word, word_to_idx)\n",
    "    train_accuracies.append(train_acc)\n",
    "    \n",
    "    pp.finalize()\n",
    "    \n",
    "    plt.plot(train_accuracies, color='dodgerblue')\n",
    "    plt.plot(val_accuracies, color='firebrick')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "# Load data\n",
    "data_matrix = read_data('examples_all.txt', 'data_all.txt')\n",
    "\n",
    "train_data, val_data = train_test_split(data_matrix, test_size=0.8)\n",
    "\n",
    "# Includes things like # of epochs, etc.\n",
    "params = {\n",
    "    'num_epochs' : 10,\n",
    "    'embedding' : bert_embedding\n",
    "}\n",
    "\n",
    "model = NeuralNetwork(hidden_size = 50)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "start_time = time.time()\n",
    "train_and_evaluate(model, train_data, val_data[0:100], optimizer, loss_function, params)\n",
    "print('Time: {}'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Testing with dropout\n",
    "model = NeuralNetwork_with_dropout(hidden_size = 50)\n",
    "\n",
    "# Choice of optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "start_time = time.time()\n",
    "train_and_evaluate(model, train_data, val_data[0:100], optimizer, loss_function, params)\n",
    "print('Time: {}'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
