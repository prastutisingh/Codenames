{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start off with replicating Jason Somers \n",
    "# Top 50000 words from glove.42B.300d.zip were written to top_50000.txt\n",
    "\n",
    "# Load libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import spatial\n",
    "\n",
    "# Load all data from massive embeddings file. You only need to do this once\n",
    "embeddings_all = {}\n",
    "with open(\"./top_50000.txt\", 'r', encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        if len(word) > 1: \n",
    "            vector = np.asarray(values[1:], \"float32\")\n",
    "            embeddings_all[word] = vector\n",
    "        \n",
    "codenames_words = []\n",
    "with open(\"./codenames_words.txt\", 'r') as f:\n",
    "    for line in f:\n",
    "        codenames_words.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose smaller sets of embeddings. \n",
    "def embeddings_size(embeddings_size, embeddings_all):\n",
    "    #embeddings_size is the length of the subset of embeddings\n",
    "    #embeddings_all is the embeddings from 50k top words\n",
    "    embeddings = {}\n",
    "    for x in list(embeddings_all)[0:embeddings_size]:\n",
    "        embeddings[x] = embeddings_all[x]\n",
    "    \n",
    "    #adds words from codenames that don't make it in the top XX words\n",
    "    for item in codenames_words:\n",
    "        if item not in embeddings:\n",
    "            embeddings[item] = embeddings_all[item]\n",
    "            \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_1000 = embeddings_size(1000, embeddings_all)\n",
    "embeddings_5000 = embeddings_size(5000, embeddings_all)\n",
    "embeddings_10000 = embeddings_size(10000, embeddings_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "def distance(embeddings, word, reference):\n",
    "    return spatial.distance.cosine(embeddings[word], embeddings[reference])\n",
    "\n",
    "def closest_words(embeddings, reference):\n",
    "    return sorted(embeddings.keys(), key=lambda w: distance(embeddings, w, reference))\n",
    "\n",
    "def goodness(embeddings, word, answers, bad):\n",
    "    if word in answers + bad: return -999\n",
    "    return sum([distance(embeddings, word, b) for b in bad]) - 4.0 * sum([distance(embeddings, word, a) \n",
    "                                                                          for a in answers])\n",
    "\n",
    "def minimax(embeddings, word, answers, bad):\n",
    "    if word in answers + bad: return -999\n",
    "    return min([distance(embeddings, word, b) for b in bad]) - max([distance(embeddings, word, a) for a in answers])\n",
    "\n",
    "def candidates(embeddings, answers, bad, size=10):\n",
    "    best = sorted(embeddings.keys(), key=lambda w: -1 * goodness(embeddings, w, answers, bad))\n",
    "    res = [(str(i + 1), \"{0:.2f}\".format(minimax(embeddings, w, answers, bad)), w) \n",
    "           for i, w in enumerate(sorted(best[:250], key=lambda w: -1 * minimax(embeddings, w, answers, bad))[:size])]\n",
    "    \n",
    "    # Modified to only return the top candidates rather than the scores since that's not important\n",
    "    return [c[2] for c in res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to help with word + scores (not necessary for data collection)...\n",
    "from itertools import zip_longest\n",
    "\n",
    "def grouper(n, iterable, fillvalue=None):\n",
    "    args = [iter(iterable)] * n\n",
    "    return zip_longest(fillvalue=fillvalue, *args)\n",
    "\n",
    "def tabulate(data):\n",
    "    data = list(grouper(10, data))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "    \n",
    "def generate_single_example(codenames_words, embeddings):\n",
    "    # Generates a single example (good words, bad words, top ten clues)\n",
    "    good = sample(codenames_words, 3)\n",
    "    bad = sample(codenames_words, 6)\n",
    "\n",
    "    clues = candidates(embeddings, good, bad)\n",
    "\n",
    "    return good, bad, clues\n",
    "\n",
    "def write_n_examples(codenames_words, embeddings, n, filename): \n",
    "    # Generates n examples and writes them to filename\n",
    "    output_file = open(filename, 'w')\n",
    "    \n",
    "    for i in range(n): \n",
    "        good, bad, clues = generate_single_example(codenames_words, embeddings)\n",
    "        word_string = ', '.join(good + bad + clues) + '\\n'\n",
    "        output_file.write(str(i) + '.' + word_string)\n",
    "    \n",
    "    output_file.close()\n",
    "    \n",
    "# write_n_examples(codenames_words, embeddings_1000, 10, 'test_examples.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read_n_examples(filename):\n",
    "    '''Reads filename of good, bad, clues examples into a three-dimensional array. \n",
    "    To access the 0th example: data[0]\n",
    "    To access the good words (in list form) from the 1st example: data[1][0]\n",
    "        good words list - 0\n",
    "        bad words list - 1\n",
    "        clue words list - 2\n",
    "    To access the first clue word from the 1st example: data[1][2][0]'''\n",
    "    file = open(filename, 'r')\n",
    "    \n",
    "    data = []\n",
    "\n",
    "    for line in file: \n",
    "        full_list = line.strip('\\n').split(',')\n",
    "        good = full_list[:3]\n",
    "        bad = full_list[3:9]\n",
    "        clues = full_list[9:]\n",
    "        data.append([good, bad, clues])\n",
    "        \n",
    "    file.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_n_examples(codenames_words, embeddings_1000, 100, 'examples_1000_vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_n_examples(codenames_words, embeddings_5000, 100, 'examples_5000_vocab.txt')\n",
    "write_n_examples(codenames_words, embeddings_10000, 100, 'examples_10000_vocab.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
