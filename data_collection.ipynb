{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start off with replicating Jason Somers \n",
    "# Top 50000 words from glove.42B.300d.zip were written to top_50000.txt\n",
    "\n",
    "# Load libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all data from massive embeddings file. You only need to do this once\n",
    "embeddings_all = {}\n",
    "with open(\"./top_50000.txt\", 'r', encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        embeddings_all[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose smaller sets of embeddings. \n",
    "def embeddings_size(embeddings_size, embeddings_all):\n",
    "    #embeddings_size is the length of the subset of embeddings\n",
    "    #embeddings_all is the embeddings from 50k top words\n",
    "    embeddings = {}\n",
    "    for x in list(embeddings_all)[0:embeddings_size]:\n",
    "        embeddings[x] = embeddings_all[x]\n",
    "    #adds words from codenames that don't make it in the top XX words\n",
    "    for item in codenames_words:\n",
    "        if item not in embeddings:\n",
    "            embeddings[item] = embeddings_all[item]\n",
    "    #\n",
    "    return embeddings\n",
    "    \n",
    "embeddings = embeddings_size(1000, embeddings_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(word, reference):\n",
    "    return spatial.distance.cosine(embeddings[word], embeddings[reference])\n",
    "\n",
    "def closest_words(reference):\n",
    "    return sorted(embeddings.keys(), key=lambda w: distance(w, reference))\n",
    "\n",
    "def goodness(word, answers, bad):\n",
    "    if word in answers + bad: return -999\n",
    "    return sum([distance(word, b) for b in bad]) - 4.0 * sum([distance(word, a) for a in answers])\n",
    "\n",
    "def minimax(word, answers, bad):\n",
    "    if word in answers + bad: return -999\n",
    "    return min([distance(word, b) for b in bad]) - max([distance(word, a) for a in answers])\n",
    "\n",
    "def candidates(answers, bad, size=10):\n",
    "    best = sorted(embeddings.keys(), key=lambda w: -1 * goodness(w, answers, bad))\n",
    "    res = [(str(i + 1), \"{0:.2f}\".format(minimax(w, answers, bad)), w) for i, w in enumerate(sorted(best[:250], key=lambda w: -1 * minimax(w, answers, bad))[:size])]\n",
    "    return [c[2] for c in res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiments\n",
    "from itertools import zip_longest\n",
    "\n",
    "def grouper(n, iterable, fillvalue=None):\n",
    "    args = [iter(iterable)] * n\n",
    "    return zip_longest(fillvalue=fillvalue, *args)\n",
    "\n",
    "def tabulate(data):\n",
    "    data = list(grouper(10, data))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "codenames_words = []\n",
    "with open(\"./codenames_words.txt\", 'r') as f:\n",
    "    for line in f:\n",
    "        codenames_words.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['dress', 'note', 'cross'],\n",
       " ['tooth', 'ham', 'truck', 'horseshoe', 'head', 'hole'])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this function takes a random samples from the codenames words\n",
    "from random import sample\n",
    "    \n",
    "def generate(codenames_words): \n",
    "    good = sample(codenames_words, 3)\n",
    "    bad = sample(codenames_words, 6)\n",
    "    \n",
    "    return good, bad\n",
    "\n",
    "generate(codenames_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['death', 'mercury', 'yard'] ['horseshoe', 'england', 'rome', 'teacher', 'net', 'ruler']\n",
      "('gas', 'parts', 'truck', 'lyrics', 'water', 'car', 'cover', 'engine', 'cause', 'oil')\n"
     ]
    }
   ],
   "source": [
    "#Script takes vocabulary of specific size, randomly generates a set of 9 words {3 good, 6 bad}, and outputs top 10 guesses\n",
    "good, bad = generate(codenames_words)\n",
    "print(good, bad)\n",
    "#clues = tabulate(candidates(answers, bad))\n",
    "clues = tabulate(candidates(good, bad))\n",
    "print(clues[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
